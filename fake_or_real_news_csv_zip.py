# -*- coding: utf-8 -*-
"""fake_or_real_news.csv.zip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JJGHUKtII4gY_JvfHcLLb_1_vdkqSd-
"""

import os
import zipfile
import pandas as pd
import numpy as np
import re
import joblib
import json
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    classification_report, confusion_matrix
)
import matplotlib.pyplot as plt
import itertools
import random

# --- Keras / TF ---
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# ---------------------
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
tf.random.set_seed(SEED)

ZIP_PATH = "/content/fake_or_real_news.csv.zip"
OUTPUT_DIR = "./models_output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------------------
# Utility: extract csv from zip
def extract_csv_from_zip(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as z:
        # find first csv file
        csv_files = [f for f in z.namelist() if f.lower().endswith('.csv')]
        if not csv_files:
            raise FileNotFoundError("Zip file doesn't contain a .csv file.")
        csv_name = csv_files[0]
        z.extract(csv_name, path=".")
        return csv_name

# ---------------------
# Basic text cleaner
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # remove urls, html tags
    text = re.sub(r'http\S+|www\.\S+', ' ', text)
    text = re.sub(r'<.*?>', ' ', text)
    # remove non-alphanumeric (keep spaces)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    # collapse whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ---------------------
# Auto-detect columns for text and label
def detect_columns(df):
    possible_label_cols = ['label','labels','target','class']
    possible_text_cols = ['text','title','article','content','news','statement']

    label_col = None
    text_col = None

    # find label
    for c in df.columns:
        if c.lower() in possible_label_cols:
            label_col = c
            break
    if label_col is None:
        # fallback: look for column with few unique values e.g. 2 classes
        for c in df.columns:
            if df[c].nunique() <= 5:
                # heuristics: contains FAKE/REAL or 0/1 or fake/real
                vals = list(df[c].dropna().astype(str).str.lower().unique())
                if any(v in ['fake','real','0','1','true','false'] for v in vals):
                    label_col = c
                    break

    # find text: prefer columns with longer average length
    best_c = None
    best_len = 0
    for c in df.columns:
        if c.lower() in possible_label_cols:
            continue
        sample = df[c].dropna().astype(str).head(100).apply(len).mean() if not df[c].dropna().empty else 0
        if sample > best_len:
            best_len = sample
            best_c = c
    text_col = best_c

    if text_col is None or label_col is None:
        raise ValueError("Could not auto-detect text and label columns. Columns found: " + ", ".join(df.columns))
    return text_col, label_col

# ---------------------
# Plot confusion matrix
def plot_confusion(cm, classes, title='Confusion matrix', cmap=None, outfile=None):
    plt.figure(figsize=(5,4))
    plt.imshow(cm, interpolation='nearest')
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    if outfile:
        plt.savefig(outfile)
    plt.show()

# ---------------------
def main():
    # 1) extract and load CSV
    csv_name = extract_csv_from_zip(ZIP_PATH)
    print("Extracted CSV:", csv_name)
    df = pd.read_csv(csv_name)
    print("Loaded dataframe shape:", df.shape)

    # 2) detect columns
    text_col, label_col = detect_columns(df)
    print("Detected text column:", text_col, "| label column:", label_col)

    # 3) Prepare text and labels
    df['text_clean'] = df[text_col].astype(str).apply(clean_text)
    raw_labels = df[label_col].astype(str).str.lower().str.strip()

    # map label values to binary 0/1 where possible
    # common: 'fake' -> 1, 'real' -> 0 (we'll map fake=1)
    label_map = {}
    if set(raw_labels.unique()) <= set(['fake','real']):
        label_map = {'real':0, 'fake':1}
    elif set(raw_labels.unique()) <= set(['0','1','2','3']):
        # if numeric-like already
        label_map = None
    else:
        # try to detect words
        if any(rv.startswith('fake') for rv in raw_labels.unique()):
            label_map = {rv: (1 if 'fake' in rv else 0) for rv in raw_labels.unique()}
        else:
            # fallback: map first unique to 0, second to 1
            uniques = list(raw_labels.unique())
            label_map = {uniques[0]:0}
            for i,u in enumerate(uniques[1:]):
                label_map[u] = i+1

    if label_map is not None:
        df['label'] = raw_labels.map(label_map).astype(int)
    else:
        df['label'] = raw_labels.astype(int)

    print("Label distribution:\n", df['label'].value_counts())

    # 4) Train/test split
    X = df['text_clean'].values
    y = df['label'].values
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)

    print("Train/Val/Test sizes:", len(X_train), len(X_val), len(X_test))

    # -----------------------------
    # Model A: TF-IDF + LogisticRegression
    # -----------------------------
    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=3)
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_val_tfidf = tfidf.transform(X_val)
    X_test_tfidf = tfidf.transform(X_test)

    # class weight
    unique, counts = np.unique(y_train, return_counts=True)
    class_weights = {int(u): float(len(y_train))/ (len(unique) * c) for u, c in zip(unique, counts)}

    print("Training Logistic Regression...")
    lr = LogisticRegression(max_iter=1000, class_weight=class_weights, random_state=SEED, solver='saga')
    lr.fit(X_train_tfidf, y_train)

    # save tfidf + lr
    joblib.dump(tfidf, os.path.join(OUTPUT_DIR, "tfidf_vectorizer.joblib"))
    joblib.dump(lr, os.path.join(OUTPUT_DIR, "logistic_regression.joblib"))
    print("Saved TF-IDF and LR models.")

    # eval LR
    val_probs_lr = lr.predict_proba(X_val_tfidf)[:,1] if lr.classes_.shape[0] > 1 else lr.predict_proba(X_val_tfidf)[:,0]
    val_preds_lr = (val_probs_lr >= 0.5).astype(int)
    print("LogReg Validation classification report:")
    print(classification_report(y_val, val_preds_lr))

    # -----------------------------
    # Model B: Keras LSTM
    # -----------------------------
    MAX_NUM_WORDS = 30000
    MAX_SEQ_LEN = 200
    EMBEDDING_DIM = 100
    BATCH_SIZE = 64
    EPOCHS = 6

    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)
    X_train_seq = tokenizer.texts_to_sequences(X_train)
    X_val_seq = tokenizer.texts_to_sequences(X_val)
    X_test_seq = tokenizer.texts_to_sequences(X_test)

    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')
    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')

    vocab_size = min(MAX_NUM_WORDS, len(tokenizer.word_index) + 1)
    print("Vocab size:", vocab_size)

    # Build LSTM
    model = Sequential([
        Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQ_LEN),
        Bidirectional(LSTM(64, return_sequences=False)),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()

    # Callbacks
    checkpoint_path = os.path.join(OUTPUT_DIR, "best_lstm.h5")
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False)
    ]

    # If labels are not binary 0/1 (multiclass), we would need different approach.
    if len(np.unique(y_train)) != 2:
        print("Warning: LSTM training expects binary labels (0/1). Current unique labels:", np.unique(y_train))
    y_train_bin = (y_train == np.unique(y_train)[1]).astype(int) if len(np.unique(y_train)) == 2 else y_train
    y_val_bin = (y_val == np.unique(y_train)[1]).astype(int) if len(np.unique(y_train)) == 2 else y_val

    # Train
    history = model.fit(
        X_train_pad, y_train_bin,
        validation_data=(X_val_pad, y_val_bin),
        epochs=EPOCHS, batch_size=BATCH_SIZE,
        callbacks=callbacks, verbose=2
    )

    # Save tokenizer and model
    joblib.dump(tokenizer, os.path.join(OUTPUT_DIR, "tokenizer.joblib"))
    model.save(os.path.join(OUTPUT_DIR, "lstm_model.h5"))
    print("Saved tokenizer and LSTM model.")

    # LSTM eval: prob predictions on validation
    val_probs_lstm = model.predict(X_val_pad, batch_size=256).reshape(-1)
    val_preds_lstm = (val_probs_lstm >= 0.5).astype(int)

    print("LSTM Validation classification report:")
    print(classification_report(y_val_bin, val_preds_lstm))

    # -----------------------------
    # Ensemble: average probabilities (LR prob + LSTM prob)
    # -----------------------------
    print("Building ensemble on validation set...")
    # Ensure shapes match
    # val_probs_lr is prob of class 1 for LR; val_probs_lstm is prob for class1
    # If label mapping was reversed, results may need interpretation. This ensemble assumes both prob align on same positive class.
    ensemble_val_probs = (val_probs_lr + val_probs_lstm) / 2.0
    ensemble_val_preds = (ensemble_val_probs >= 0.5).astype(int)

    print("Ensemble Validation classification report:")
    try:
        print(classification_report(y_val_bin, ensemble_val_preds))
    except Exception as e:
        print("Could not print classification report for ensemble:", e)

    # -----------------------------
    # Final evaluation on test set
    # -----------------------------
    print("Evaluating on test set...")

    # LR test probs
    test_probs_lr = lr.predict_proba(X_test_tfidf)[:,1]
    test_preds_lr = (test_probs_lr >= 0.5).astype(int)

    # LSTM test probs
    test_probs_lstm = model.predict(X_test_pad, batch_size=256).reshape(-1)
    test_preds_lstm = (test_probs_lstm >= 0.5).astype(int)

    # Ensemble
    test_ensemble_probs = (test_probs_lr + test_probs_lstm) / 2.0
    test_ensemble_preds = (test_ensemble_probs >= 0.5).astype(int)

    # Compute metrics helper
    def print_metrics(y_true, y_pred, probs=None, name="Model"):
        print(f"--- {name} ---")
        print("Accuracy:", accuracy_score(y_true, y_pred))
        print("Precision:", precision_score(y_true, y_pred, zero_division=0))
        print("Recall:", recall_score(y_true, y_pred, zero_division=0))
        print("F1:", f1_score(y_true, y_pred, zero_division=0))
        if probs is not None:
            try:
                print("ROC AUC:", roc_auc_score(y_true, probs))
            except Exception:
                pass
        print(classification_report(y_true, y_pred))
        cm = confusion_matrix(y_true, y_pred)
        plot_confusion(cm, classes=[str(c) for c in np.unique(y_true)], title=f"{name} Confusion Matrix",
                       outfile=os.path.join(OUTPUT_DIR, f"cm_{name.replace(' ','_')}.png"))

    # Depending on label bin mapping: y_test_bin:
    if len(np.unique(y_train)) == 2:
        # compute y_test_bin using same positive label as training
        positive_label = np.unique(y_train)[1]
        y_test_bin = (y_test == positive_label).astype(int)
    else:
        y_test_bin = y_test

    print_metrics(y_test_bin, test_preds_lr, probs=test_probs_lr, name="Logistic Regression")
    print_metrics(y_test_bin, test_preds_lstm, probs=test_probs_lstm, name="LSTM")
    print_metrics(y_test_bin, test_ensemble_preds, probs=test_ensemble_probs, name="Ensemble (avg)")

    # Save final ensemble metadata
    meta = {
        "label_mapping": label_map,
        "tfidf_path": os.path.join(OUTPUT_DIR, "tfidf_vectorizer.joblib"),
        "lr_path": os.path.join(OUTPUT_DIR, "logistic_regression.joblib"),
        "tokenizer_path": os.path.join(OUTPUT_DIR, "tokenizer.joblib"),
        "lstm_path": os.path.join(OUTPUT_DIR, "lstm_model.h5"),
        "vocab_size": int(vocab_size),
        "max_seq_len": int(MAX_SEQ_LEN)
    }
    with open(os.path.join(OUTPUT_DIR, "model_metadata.json"), "w") as f:
        json.dump(meta, f, indent=2)

    print("All done. Models and outputs saved to", OUTPUT_DIR)

if __name__ == "__main__":
    main()